{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Identify the model approach(es), describe, and justify the selection\n",
    "\n",
    "Support vector machines (SVM) are a type of supervised learning used for classification and regression. Here, SVMs will be used to classify patients as either readmission (readmitted within 30 days of being discharged) or non_readmission. SVMs try to find a hyperplane (decision boundary) that separates classes of observations in feature space. Unlike probability models, the SVM does not use probability for classification; instead, we aim for the direct caclulation of a separating hyperplane (as in notes). Moreover, SVMs are a type of *large margin* classifiers. In these types of classifiers, we try to find the best separating hyperplane that is farthest as possible from any points. In other words, we want to minimize the norm of the parameter vector by choosing a $\\theta$ such that the projection of each point x onto $\\theta$ is a maximum.\n",
    "\n",
    "Support vector machines are useful in this problem because they work well in high dimensional spaces, and we have multiple variables that we want to use to predict readmission. Even though they can use many features accurately, they are also memory efficient; SVMS only use a subset of training points in the decision function (i.e., the points \"closest\" to the decision boundary line, because those that lie farther from the boundary are easy to classify). Many tuning parameters are available for SVMS, including different kernels and regularization terms that can account for overfitting and bias errors. I will test multiple different kernel possibilities, and then investigate the regularization term. I expect that I will need something more nuanced than a basic Gaussian or linear kernel, and I predict that the most accurate models with come from polynomial kernels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd \n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.cm as cmap\n",
    "sns.set_context('talk')\n",
    "import seaborn as sns; sns.set_context('notebook')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import brier_score_loss, make_scorer\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code, parameterize, and run model (including visualization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the simplest SVM model: a linear kernel, and no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/sarahmaddox/Desktop/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X = pd.read_csv(data_path+\"x_data.csv\", index_col = \"patient_id\")\n",
    "del X['Unnamed: 0']\n",
    "y_data = pd.read_csv(data_path+\"y_data.csv\")\n",
    "y=y_data.pop(\"readmitted_true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear')\n",
    "svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71825000000000006"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "# Create color maps for 3-class classification problem, as with iris\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "def plot_estimator(estimator, X, y, ax=None):\n",
    "    \n",
    "    try:\n",
    "        X, y = X.values, y.values\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    \n",
    "    estimator.fit(X, y)\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_estimator(svc, X[['bmi_mean','bmi_std']], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        X.values, y.values, test_size=0.4, random_state=0)\n",
    "\n",
    "#5 fold cross-validation, 5 way partition \n",
    "#you can see you get a really good score for some but not the others \n",
    "scores = model_selection.cross_val_score(svc, X.values, y.values, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_train = svc.predict(X_train)\n",
    "pred_test = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>3245</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1224</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0.0  1.0\n",
       "Actual              \n",
       "0.0        3245  116\n",
       "1.0        1224  215"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_train, pred_train, \n",
    "            rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does a fairly bad job at predicting when the actual is 1; it is more often predicted as 0 even if the actual value is one (1224 versus 215). The model is probably biased towards predicting that the patient is not readmitted within 30 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Goodness of fit assessments, performance characteristics (including visualization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improvements to model/tuning of parameters; model selection methods, justification of improvements/tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "C corresponds to the inverse of the regularization parameter. The choice of C will either help reduce bias, reduce variance, or something in the middle:\n",
    "large C = low bias, high variance\n",
    "small C = high bias, low variance\n",
    "In an SVM, a lot of regularization means that the model will have a \"soft margin\" that allows some points to cross the optimal decision boundary and get misclassified. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison of models; identification of best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Implications of model and conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
